---
title: "CS_LAB4"
author: "Damian Ke & Kyriakos Papadopoulos"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1: Computations with Metropolis–Hastings

## Question 1 

Use Metropolis–Hastings algorithm to generate samples from this distribution by usingproposal distribution as log–normal LN(Xt, 1), take some starting point. Plot the chain you obtained as a time series plot. What can you guess about the convergence of the chain? If there is a burn–in period, what can be the size of this period?

**Answer 1: **

```{r, include=FALSE}
set.seed(12345)
# Proposal distribution
p_x <- function(x){
  out <- x ^ 5 *  exp(-x)
  return(out)
}
```

```{r}
f.MCMC.MH<-function(nstep, X0, props){
  vN<-1:nstep
  vX<-rep(X0,nstep)
  
  for (i in 2:nstep){
    
    X<-vX[i-1]
    Y<-rlnorm(1, meanlog= log(X), sdlog=props)
    u<-runif(1)
    a <- min(c(1, (p_x(Y)*dlnorm(X,meanlog=log(Y),sdlog=props))/
                 (p_x(X)*dlnorm(Y, meanlog= log(X),sdlog=props))))
    if (u <=a){
      vX[i]<-Y
      }
    else{
      vX[i]<-X
      }    
  
  }
   
  plot(vN,vX,pch=19,cex=0.3,col="black",xlab="t",ylab="X(t)")
  return(vX)
}
X = f.MCMC.MH(2000, 100, 1)
```

We can clearly observe that our graph converge (imprecise. we do expect a clearer answer here. what exactly is converging (surely not the graph!)? and to what is it converging?). The burn-in period is not clear in that case but we could say that it's from the beginning of the graph up to a value a quite bigger than 0 (if you can't see a burn-in period in the graph clearly, feel free to burn-in a few hundred samples. for e.g., for example t = 250 iterations seems to be an acceptable burn-in period. if you want fewer than that, examine the data to arrive at a better figure.), as we can see in the graph.

## Question 2
(OK but please provide a more precise explanation regarding the convergence of chains)
Perform Step 1 by using the chi–square distribution $x^{2}([X_t + 1])$ as a proposal distribution, where $[x]$ is the floor function, meaning the integer part of x for positive x, i.e. $[2.95] = 2$

**Answer 2: **

```{r}
f.MCMC.MH2<-function(nstep, X0){
  vN<-1:nstep
  vX<-rep(X0,nstep)
  
  for (i in 2:nstep){
    
    X<-vX[i-1]
    Y<-rchisq(1, df=floor(X + 1))
    u<-runif(1)
    a <- min(c(1, (p_x(Y)*dchisq(X, df=floor(X + 1)))/(p_x(X)*dchisq(Y, df=floor(Y + 1)))))
    
    if (u <=a){
      vX[i]<-Y
    }
    else{
      vX[i]<-X
    }    
    
  }
   
  plot(vN,vX,pch=19,cex=0.3,col="black",xlab="t",ylab="X(t)",main="
       ")
  return(vX)
}
x <- f.MCMC.MH2(2000, 100)
```

We can clearly see that the samples with chi-square converge as all(all? or all after burn-in?) the values are centered around (approximately) 5 as we can see from the graph. We can find the burn-in period visually and say that the burn-in period is from 0 up to a a value a little bigger than 0 compared to 2000. The burn-in period is the period required for a function to reach a stable operating state(imprecise), so the burn-in period finishes when the function(again, the function does not converge. please clarify your statements) starts converging. 

## Question 3 

Compare the results of Steps 1 and 2 and make conclusions.


**Answer 3: ** In both cases the samples converge and we can get a representive sample of the proposal distribution. Also the burn-in period in both cases is small.


## Question 4

Generate 10 MCMC sequences using the generator from Step 2 and starting points 1, 2, ..., or 10. Use the Gelman–Rubin method to analyze convergence of these sequences. 

**Answer 4: **
```{r, fig.show='hide', warning=FALSE}
library(coda)
k<-10
## Run 10 sequences with 10 different starting points and use them in if
f1 <- mcmc.list()
for (i in 1:k)
  { 
    x <- f.MCMC.MH2(1000, i)
    f1[[i]]<-as.mcmc(x)
    
 }
print(gelman.diag(f1))
```
As we can see from Gelman–Rubin method our result is a value very close to 1 and indicates that chains have converged.

## Question 5

Estimate $\int_{0}^{\infty}xf(x)$ using the samples from Steps 1 and 2


**Answer 5: **

The integral $\int_{0}^{\infty}xf(x)$ is known that it's the mean value of f(x). So we can easily find the integral by calculating the mean value of the samples that we get.

```{r, fig.show='hide', echo=FALSE}
cat("The integral from the first sample is ", mean(f.MCMC.MH(1000, 1, 1)), "\n")
cat("The integral from the second sample is ", mean(f.MCMC.MH2(2000, 100)))
```

## Question 6:

The distribution generated is in fact a gamma distribution. Look in the literature and define the actual value of the integral. Compare it with the one you obtained.

**Answer 6: **

From the literature we know that a gamma distribution has the following form $$f(x) = [\frac{1}{\Gamma(\alpha)\beta^{\alpha}}] x^{\alpha - 1}e^{\frac{-x}{\beta}}$$ So, we can easily conclude that $\alpha=6$ and $\beta=1$. 

The mean value from a Gamma distribution is $\alpha\beta$, so in our case is 6*1 = 1. Compared to our answers in 5th question we can observe that we calculated  the mean value right as our samples were accurate in both cases

# Question 2 Gibbs sampling

## 1
What kind of model is reasonable to use here?
```{r, warning=FALSE}
load("chemical.RData")
library(ggplot2)
data= data.frame(X,Y)
ggplot(data, aes(x=X,y=Y))+
  geom_point()
```

**Answer**
As there seem to be a correlation between X and Y. A linear or polynomial regression can be a reasonable
model to use here.

## 2
Present the formulae showing the likelihood $p(\overrightarrow{Y}\mid\overrightarrow{\mu})$ and the prior $p(\overrightarrow{\mu})$

**Answer**
We assume that Normal distribution takes in $N(\mu,\sigma^2)$

$$p(\overrightarrow{\mu}) = 1* N(\mu_1,0.2)*N(\mu_2,0.2)*N(\mu_3,0.2)*...*N(\mu_{n-1},0.2)$$
Which is equal to $p(\overrightarrow{\mu}) = (\frac{1}{ \sigma\sqrt{2*\pi}})^{n-1}*e^{(-\frac{\sum_{i=2}^{n}(\mu_{i}-\mu_{i-1})^2}{2*\sigma^2})}$

$$p(\overrightarrow{Y}\mid\overrightarrow{\mu})= (\frac{1}{\sigma\sqrt{2*\pi}})^n*e^{(-\frac{\sum_{i=1}^{n}({y_i}-\mu_{i})^2}{2*\sigma^2})} $$ 






## 3
Use Bayes' Theorem to get the posterior up to a constant proportionality, and then find
out the distributions of $p({\mu_i}\mid\overrightarrow\mu_{-i},\overrightarrow{Y})$, where $\mu_{-i}$ is a vector containing all $\mu$ values except of $\mu_i$

**Answer**
To get posterior 
$$p({\overrightarrow\mu}\mid\overrightarrow{Y}) \propto p(\overrightarrow{Y}\mid\overrightarrow{\mu}) * p(\overrightarrow{\mu})$$

It is also important to mention that
$p({\overrightarrow\mu},\overrightarrow{Y}) = p(\overrightarrow{Y}\mid\overrightarrow{\mu}) * p(\overrightarrow{\mu})$ as they are dependent on each other.

Therefore, $p({\mu_n}\mid\overrightarrow{\mu}_{-n},\overrightarrow{Y}) = \frac{p(\overrightarrow{\mu}_{-n},\overrightarrow{Y},\mu_n)}{p(\overrightarrow{\mu}_{-n},\overrightarrow{Y})}$
and $p(\overrightarrow{\mu}_{-n},\overrightarrow{Y},\mu_n) = p({\overrightarrow\mu},\overrightarrow{Y})$.

$$p(\overrightarrow{\mu}\mid\overrightarrow{Y}) \propto e^{(-\frac{\sum_{i=1}^{n}({y_i}-\mu_{i})^2}{2\sigma^2})}* e^{(-\frac{\sum_{i=1}^{n-1}({\mu_{i+1}}-\mu_{i})^2}{2\sigma^2})}$$
Multiplication of these two negative exponents moves places of (a-x) to (x-a).
(BE MORE PRECISE!!)
Due to $\frac{p(\overrightarrow{\mu}_{-n},\overrightarrow{Y},\mu_n)}{p(\overrightarrow{\mu}_{-n},\overrightarrow{Y})}$ The $\Sigma$ will be canceled out as the only part being left is $\mu_n$.

For case when 1:
$p(\mu_1\mid\overrightarrow{\mu_{-1}},\overrightarrow{Y})\propto e^{(-\frac{((\mu_1-y_1)^2+(\mu_1-\mu_2)^2)}{2\sigma^2})}$
(how did you get to this? a simple explanation would do.)
and through Hint B
$$e^{(-\frac{({\mu_1}-(y_1+\mu_{2})/2)^2+}{2\sigma^2/2})} \sim N(\frac{y_1+\mu_2}{2},\frac{\sigma^2}{2})$$


For case when n:
$p(\mu_n\mid\overrightarrow\mu_{-n},\overrightarrow{Y}) \propto e^{(-\frac{(\mu_{n}-{y_n})^2+(\mu_n-\mu_{n-1})^2}{2\sigma^2})}$ 
and through Hint B
$$e^{(-\frac{({\mu_n}-(y_n+\mu_{n-1})/2)^2+}{2\sigma^2/2})} \sim N(\frac{y_n+\mu_{n-1}}{2},\frac{\sigma^2}{2})$$


For cases when i(WHAT IS I):
The exponent from the prior has additional case for i as there is a previous and next step.
$p({\mu_i}\mid\overrightarrow\mu_{-i},\overrightarrow{Y}) \propto e^{(-\frac{({y_i}-\mu_{i})^2+(\mu_i-\mu_{i-1})^2+(\mu_{i}-\mu_{i+1})^2}{\sigma^2})}$ 
and through Hint C
$$e^{(-\frac{({\mu_i}-(y_i+\mu_{i-1}+\mu_{i+1})/3)^2+}{2\sigma^2/3})} \sim N(\frac{y_i+\mu_{i-1}+\mu_{i+1}}{2},\frac{\sigma^2}{3})$$

## 4
Plot the expected value
of $\overrightarrow\mu$ versus X and Y versus X in the same graph. Does it seem that you have managed to
remove the noise? Does it seem that the expected value of $\overrightarrow\mu$ can catch the true underlying
dependence between Y and X?

It is important to mention that function rnorm takes normal distribution as $N(\mu,\sigma)$.
```{r, warning=FALSE}
Gibbs<-function(nstep,vmean,mVar){
  n<-length(vmean)
  mX<-matrix(0,nrow=nstep,ncol=n)
  #Starting point
  mX[1,] = rep(0,n)
  
  #Loop to get 1000 values
  for (x in 2:nstep){
    #Case i = 1
    mX[x,1] = rnorm(1,mean=(vmean[1]+mX[x-1,2])/2,sd=sqrt((mVar/2)))
    #For case i
    for (i in 2:(n-1)){
      mX[x,i] = rnorm(1,mean=(vmean[i]+mX[x,i-1]+mX[x-1,i+1])/3,sd=sqrt((mVar/3)))
    }
    #Case i = n
    mX[x,n] = rnorm(1,mean=(vmean[n]+mX[x,n-1])/2,sd=sqrt((mVar/2)))
  }
  mX
}
set.seed(12345)
results<-Gibbs(1000,Y,0.2)
library(ggplot2)
mean_results = colMeans(results)
data["mean_results"] = mean_results
p = ggplot()+
  geom_line(aes(x=X,y=mean_results), color="red")+
  geom_line(aes(x=X,y=Y),color="blue")+
  ylab("Y")
p
```
(what are the two lines? show legend)
There seem to be no noise(relatively less noise would be better) as the line corresponding for $\overrightarrow\mu$ is smooth. 
It seems that the expected value of $\overrightarrow\mu$ catches the true underlying dependence between Y and X as both of the lines are having almost the same values.


## 5
Comment on the burn-in period and convergence.
```{r}
results_df = data.frame(1:1000,results[,50])
colnames(results_df) = c("ID", "Results")
p2 = ggplot(results_df,aes(x=ID,y=Results))+
  geom_line()+
  xlab("nstep")
p2
```

**Answer**
For the beginning of the trace plot, there exist a small burn-in period.
Thereafter, the plot converges(again imprecise).

# Appendix
```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```